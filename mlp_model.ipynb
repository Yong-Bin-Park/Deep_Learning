{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ba8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run mathutil.ipynb\n",
    "np.random.seed(1234)       #난수발생패턴 고정\n",
    "def randomize(): np.random.seed(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f35fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    #객체 초기화\n",
    "    def __init__(self, name, dataset):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        self.is_training = False\n",
    "        if not hasattr(self, 'rand_std'): self.rand_std = 0.030\n",
    "    \n",
    "    #출력문으로 객체를 출력할 때의 출력 문자열 생성 방법을 정의한다.\n",
    "    def __str__(self):\n",
    "        return '{}/{}'.format(self.name, self.dataset)\n",
    "\n",
    "    #전체 과정을 실행시키는 메인 함수 역할(학습, 평가, 시각화 메서드를 차례로 호출)\n",
    "    def exec_all(self, epoch_count=10, batch_size=10, learning_rate=0.001,\n",
    "                 report=0, show_cnt=3):\n",
    "        self.train(epoch_count, batch_size, learning_rate, report)\n",
    "        self.test()\n",
    "        if show_cnt > 0: self.visualize(show_cnt) #시각화 메서드는 시각화 출력 대상 데이터 수가 0보다 클 떄만 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c043d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpModel(Model):\n",
    "    def __init__(self, name, dataset, hconfigs):\n",
    "        #기반 클래스인 Model 클래스를 찾아 그 객체 초기화 함수를 호출하여 name, dataset값을 저장한다.\n",
    "        super(MlpModel, self).__init__(name, dataset)\n",
    "        #신경망이 이용할 파라미터를 준비\n",
    "        self.init_parameters(hconfigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e911b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_init_parameters(self, hconfigs):\n",
    "    self.hconfigs = hconfigs\n",
    "    self.pm_hiddens = []\n",
    "\n",
    "    #입출력 벡터 크기는 전역변수가 아니라 dataset 객체의 속성값으로부터 얻는다\n",
    "    prev_shape = self.dataset.input_shape\n",
    "\n",
    "    for hconfig in hconfigs:\n",
    "        pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig)\n",
    "        #생성된 파라미터들을 전역 변수에 저장하는 대신 객체 변수로 저장한다.\n",
    "        self.pm_hiddens.append(pm_hidden)\n",
    "\n",
    "    output_cnt = int(np.prod(self.dataset.output_shape))\n",
    "    self.pm_output, _ = self.alloc_layer_param(prev_shape, output_cnt)\n",
    "        \n",
    "def mlp_alloc_layer_param(self, input_shape, hconfig):\n",
    "    input_cnt = np.prod(input_shape)\n",
    "    output_cnt = hconfig\n",
    "\n",
    "    weight, bias = self.alloc_param_pair([input_cnt, output_cnt])\n",
    "\n",
    "    return {'w':weight, 'b':bias}, output_cnt\n",
    "\n",
    "def mlp_alloc_param_pair(self, shape):\n",
    "    weight = np.random.normal(0, self.rand_std, shape)\n",
    "    bias = np.zeros([shape[-1]])\n",
    "    return weight, bias\n",
    "\n",
    "#이 코드에서 정의된 함수들을 클래스의 멤버 함수, 즉 메서드로 등록한다.\n",
    "#이때 정의된 함수 이름과 등록되는 메서드 이름이 서로 다른데 이처럼 두 이름을 독립적으로 부여하면 파생 클래스에서\n",
    "#메서드를 재정의할때 함수 이름을 달리 할 수 있어서 프로그램의 가독성이 높아짐\n",
    "MlpModel.init_parameters = mlp_init_parameters\n",
    "MlpModel.alloc_layer_param = mlp_alloc_layer_param\n",
    "MlpModel.alloc_param_pair = mlp_alloc_param_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5d1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_train(self, epoch_count=10, batch_size=10, \\\n",
    "                    learning_rate=0.001, report=0):\n",
    "    #학습률 파라미터를 객체 변수를 이용해 전달\n",
    "    self.learning_rate = learning_rate\n",
    "    \n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    time1 = time2 = int(time.time())\n",
    "    if report != 0:\n",
    "        print('Model {} train started:'.format(self.name))\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        #데이터 뒤섞기 기능을 따로 독립시켜 호출\n",
    "        self.dataset.shuffle_train_data(batch_size*batch_count)\n",
    "        for n in range(batch_count):\n",
    "            trX, trY = self.dataset.get_train_data(batch_size, n)\n",
    "            cost, acc = self.train_step(trX, trY)\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "\n",
    "        if report > 0 and (epoch+1) % report == 0:\n",
    "            #get_validate_data()메서드를 호출해 그때그때 새로 받아오는 제한된 수의 검증용 데이터를 이용한다.\n",
    "            vaX, vaY = self.dataset.get_validate_data(100)\n",
    "            acc = self.eval_accuracy(vaX, vaY)\n",
    "            time3 = int(time.time())\n",
    "            tm1, tm2 = time3-time2, time3-time1\n",
    "            #확장에 대비해 검증 단계 출력을 데이터셋의 train_prt_result() 메서드에 의뢰해 생성한다.\n",
    "            #실행 시간 추적 정보를 전달하여 출력에 반영할 수 있게 했다.\n",
    "            self.dataset.train_prt_result(epoch+1, costs, accs, acc, tm1, tm2)\n",
    "            time2 = time3\n",
    "\n",
    "    tm_total = int(time.time()) - time1\n",
    "    print('Model {} train ended in {} secs:'.format(self.name, tm_total))\n",
    "    \n",
    "MlpModel.train = mlp_model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50364910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exec_all()메서드가 평가를 위해 호출하는 메서드\n",
    "def mlp_model_test(self):\n",
    "    teX, teY = self.dataset.get_test_data()\n",
    "    time1 = int(time.time())\n",
    "    acc = self.eval_accuracy(teX, teY)\n",
    "    time2 = int(time.time())\n",
    "    self.dataset.test_prt_result(self.name, acc, time2-time1)\n",
    "\n",
    "MlpModel.test = mlp_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e43cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시각화 메서드 정의\n",
    "def mlp_model_visualize(self, num):\n",
    "    print('Model {} Visualization'.format(self.name))\n",
    "    #데이터셋에서 시각화용 데이터를 얻은 후\n",
    "    deX, deY = self.dataset.get_visualize_data(num)\n",
    "    #get_estimate()메서드를 실행시켜 신경망 추정 정보를 얻는다.\n",
    "    est = self.get_estimate(deX)\n",
    "    #신경망 추정 정보를 데이터셋의 시각화 메서드에 제공해 시각화 출력을 생성하며 이때 원래의 입력 및 출력 정보도 함께 제공\n",
    "    self.dataset.visualize(deX, est, deY)\n",
    "\n",
    "MlpModel.visualize = mlp_model_visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b62b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#미니배치 학습 메서드 정의\n",
    "#is_training 플래그의 제어 기능이 추가된 점만 다름, 이 플래그는 train()메서드가 아니라 이 메서드에서 제어해야함\n",
    "\n",
    "def mlp_train_step(self, x, y):\n",
    "    self.is_training = True\n",
    "    \n",
    "    output, aux_nn = self.forward_neuralnet(x)\n",
    "    loss, aux_pp = self.forward_postproc(output, y)\n",
    "    accuracy = self.eval_accuracy(x, y, output)\n",
    "    \n",
    "    G_loss = 1.0\n",
    "    G_output = self.backprop_postproc(G_loss, aux_pp)\n",
    "    self.backprop_neuralnet(G_output, aux_nn)\n",
    "\n",
    "    self.is_training = False\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "MlpModel.train_step = mlp_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef58008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망 부분에 대한 순전파 메서드 정의\n",
    "def mlp_forward_neuralnet(self, x):\n",
    "    hidden = x\n",
    "    aux_layers = []\n",
    "\n",
    "    for n, hconfig in enumerate(self.hconfigs):\n",
    "        hidden, aux = self.forward_layer(hidden, hconfig, self.pm_hiddens[n])\n",
    "        aux_layers.append(aux)\n",
    "\n",
    "    #출력 계층은 hconfig 인숫값 대신 None을 전달하는 방법으로 출력 계층임을 나타내도록 함\n",
    "    output, aux_out = self.forward_layer(hidden, None, self.pm_output)\n",
    "    \n",
    "    #리턴값을 아래 역전파 메서드에 전달\n",
    "    return output, [aux_out, aux_layers]\n",
    "\n",
    "#신경망 부분에 대한 역전파 메서드 정의\n",
    "def mlp_backprop_neuralnet(self, G_output, aux):\n",
    "    aux_out, aux_layers = aux\n",
    "    \n",
    "    G_hidden = self.backprop_layer(G_output, None, self.pm_output, aux_out)\n",
    "    \n",
    "    for n in reversed(range(len(self.hconfigs))):\n",
    "        hconfig, pm, aux = self.hconfigs[n], self.pm_hiddens[n], aux_layers[n]\n",
    "        G_hidden = self.backprop_layer(G_hidden, hconfig, pm, aux)\n",
    "        \n",
    "    return G_hidden\n",
    "\n",
    "MlpModel.forward_neuralnet = mlp_forward_neuralnet\n",
    "MlpModel.backprop_neuralnet = mlp_backprop_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99e3dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#계층 하나에 대한 순전파 메서드 정의\n",
    "def mlp_forward_layer(self, x, hconfig, pm):\n",
    "    #선형 연산\n",
    "    y = np.matmul(x, pm['w']) + pm['b']\n",
    "    #비선형 활성화 함수 관련처리 -> 은닉 계층에서만 적용(은닉 계층인지 출력 계층인지는 hconfig 인숫값이 None인지를 검사)\n",
    "    if hconfig is not None: y = relu(y)\n",
    "    return y, [x,y]\n",
    "\n",
    "#계층 하나에 대한 역전파 메서드 정의\n",
    "def mlp_backprop_layer(self, G_y, hconfig, pm, aux):\n",
    "    x, y = aux\n",
    "    #비선형 활성화 함수 관련처리 -> 은닉 계층에서만 적용(은닉 계층인지 출력 계층인지는 hconfig 인숫값이 None인지를 검사)\n",
    "    if hconfig is not None: G_y = relu_derv(y) * G_y\n",
    "    \n",
    "    #선형 연산에 대한 역전파 처리\n",
    "    g_y_weight = x.transpose()\n",
    "    g_y_input = pm['w'].transpose()\n",
    "    \n",
    "    G_weight = np.matmul(g_y_weight, G_y)\n",
    "    G_bias = np.sum(G_y, axis=0)\n",
    "    G_input = np.matmul(G_y, g_y_input)\n",
    "    \n",
    "    pm['w'] -= self.learning_rate * G_weight\n",
    "    pm['b'] -= self.learning_rate * G_bias\n",
    "\n",
    "    return G_input\n",
    "\n",
    "MlpModel.forward_layer = mlp_forward_layer\n",
    "MlpModel.backprop_layer = mlp_backprop_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17be2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#후처리 부분에 대한 순전파 메서드 정의\n",
    "def mlp_forward_postproc(self, output, y):\n",
    "    #데이터셋 객체의 메서드를 호출해 손실 함숫값을 실제로 계산하도록 요청\n",
    "    #이때, 역전파용 보조 정보를 aux_loss에 챙겨두었다가 역전파때 제공\n",
    "    loss, aux_loss = self.dataset.forward_postproc(output, y)\n",
    "    #forward_extra_cost()메서드를 호출해 추가적인 별도의 손실 성분 loss_extra와 역전파용 보조 정보 aux_extra를 보고한다.\n",
    "    extra, aux_extra = self.forward_extra_cost(y)\n",
    "    return loss + extra, [aux_loss, aux_extra]\n",
    "\n",
    "def mlp_forward_extra_cost(self, y):\n",
    "    #0,None이라는 의미 없는 값은 아직 아무런 정규화 장치도 도입되지 않아 계산에 반영할 추가 손실 성분이 없기 때문\n",
    "    return 0, None\n",
    "\n",
    "MlpModel.forward_postproc = mlp_forward_postproc\n",
    "MlpModel.forward_extra_cost = mlp_forward_extra_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc594d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#후처리 부분에 대한 역전파 메서드 정의\n",
    "def mlp_backprop_postproc(self, G_loss, aux):\n",
    "    aux_loss, aux_extra = aux\n",
    "    #순전파때 호출한 forward_extra_cost()메서드와 짝을 이루는 메서드로서 정규화 기법을 도입할 때를 대비해 준비되었지만\n",
    "    #당장은 아무러 할일이 없으므로 아래 정의된 pass로 빈 함수 형태로 정의\n",
    "    self.backprop_extra_cost(G_loss, aux_extra)\n",
    "    #loss의 손실 기울기 G_loss로부터 신경망 출력 output의 손실 기울기 G_output을 계산\n",
    "    G_output = self.dataset.backprop_postproc(G_loss, aux_loss)\n",
    "    return G_output\n",
    "\n",
    "def mlp_backprop_extra_cost(self, G_loss, aux):\n",
    "    pass\n",
    "\n",
    "MlpModel.backprop_postproc = mlp_backprop_postproc\n",
    "MlpModel.backprop_extra_cost = mlp_backprop_extra_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed5dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#정확도 계산 메서드 정의\n",
    "def mlp_eval_accuracy(self, x, y, output=None):\n",
    "    if output is None:\n",
    "        #이를 검사하여 값 지정이 누락된 경우 forward_neuralnet()메서드를 호출해 직접 순전파 처리를 수행\n",
    "        output, _ = self.forward_neuralnet(x)\n",
    "    #추정의 정확도를 계산하는 구체적인 과정은 데이터셋 객체의 메서드에 맡겨 처리\n",
    "    accuracy = self.dataset.eval_accuracy(x, y, output)\n",
    "    return accuracy\n",
    "\n",
    "MlpModel.eval_accuracy = mlp_eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d83859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#추정 결과 산출 메서드 정의\n",
    "#신경망이 로짓값이나 로짓값 벡터 등으로 구해낸 결과를 x값으로 전달받아 확률값이나 확률 분포 등 출력 유형에 따른\n",
    "#의미 있는 값으로 바꾸어 estimate 변수에 담아 보고하는 것이 이 함수의 역할이다.\n",
    "def mlp_get_estimate(self, x):\n",
    "    output, _ = self.forward_neuralnet(x)\n",
    "    #시각화에 적합한 형태의 추정 결과를 얻어내는 구체적인 과정은 데이터셋 객체의 메서드를 호출해 처리\n",
    "    estimate = self.dataset.get_estimate(output)\n",
    "    return estimate\n",
    "\n",
    "MlpModel.get_estimate = mlp_get_estimate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
